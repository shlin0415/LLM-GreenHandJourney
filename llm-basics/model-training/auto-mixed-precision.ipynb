{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto mixed precision    \n",
    "Core idea: BF16 Exponent Bits 8 bits (Same as FP32)    \n",
    "\n",
    "https://docs.qq.com/doc/DSUtuWFREUEFCbHZQ    \n",
    "2.2.2\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "This guide covers things about modern Automatic Mixed Precision (AMP), from the fundamentals of data types to advanced industry standards like Megatron and FP8.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Introduction to Modern AMP\n",
    "\n",
    "### What is the Core Idea?\n",
    "\n",
    "The core idea of **AMP** is to use **different numerical precisions** for different operations during training to get the best of both worlds:\n",
    "\n",
    "1. **Speed & Memory Efficiency:** Use lower precision (16-bit) for computationally heavy tasks like Matrix Multiplications ().\n",
    "2. **Numerical Stability:** Use full precision (32-bit) for sensitive operations like Reductions () and for storing the \"Master Weights.\"\n",
    "\n",
    "By doing this, you can often **double your training speed** and **halve your memory usage** without losing model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: FP16 vs. BF16\n",
    "\n",
    "Modern deep learning has shifted toward **BF16** (Brain Float 16) because it is much more robust for training than the older **FP16**.\n",
    "\n",
    "| Feature | FP16 (Standard Half) | BF16 (Brain Float) |\n",
    "| --- | --- | --- |\n",
    "| **Exponent Bits** | 5 bits (Small range) | 8 bits (**Same as FP32**) |\n",
    "| **Mantissa Bits** | 10 bits (Higher precision) | 7 bits (Lower precision) |\n",
    "| **Max Value** | ~65,500 | ~ |\n",
    "| **Gradient Scaling** | **Mandatory** (Avoids underflow) | **Optional/Unnecessary** |\n",
    "| **Hardware** | Most GPUs (Volta+) | Modern GPUs (Ampere A100/30-series+) |\n",
    "\n",
    "#### When to use BF16?\n",
    "\n",
    "* **Always** if your hardware supports it (NVIDIA Ampere architecture or newer, like A100, H100, RTX 30/40 series).\n",
    "* When training **Large Language Models (LLMs)**, as they are prone to \"spiky\" gradients that cause FP16 to overflow/diverge.\n",
    "\n",
    "#### Does BF16 need adjustments (like Learning Rate)?\n",
    "\n",
    "* **Learning Rate:** Generally, you **do not** need to change your learning rate when switching from FP32 to BF16. It is much more \"plug-and-play\" than FP16.\n",
    "* **Epsilon ():** In optimizers like Adam, you might need to increase the epsilon value (e.g., from `1e-8` to `1e-5`) because the lower precision of the mantissa in BF16 can make very small numbers indistinguishable from zero.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† How AMP Influences Model Training\n",
    "\n",
    "1. **Memory Footprint:** You can fit **larger models** or **larger batch sizes** on the same GPU.\n",
    "2. **Throughput:** Training is significantly faster because modern Tensor Cores are optimized for 16-bit math.\n",
    "3. **Stability:** * **FP16** requires a `GradScaler` to prevent gradients from becoming zero (underflow).\n",
    "* **BF16** behaves almost exactly like FP32 training, making it highly stable.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üè¢ Advanced Industry Methods\n",
    "\n",
    "### 1. AMP in Megatron (NVIDIA)\n",
    "\n",
    "**Megatron-LM** is a framework for training massive models. Its AMP implementation is highly optimized:\n",
    "\n",
    "* **Master Weights:** It maintains a main copy of weights in `FP32` and a \"buffer\" copy in `FP16/BF16` for the actual forward/backward pass.\n",
    "* **Distributed Optimizer:** It integrates AMP with data/model parallelism, ensuring that gradient synchronization (All-Reduce) happens in lower precision to save bandwidth, while weight updates happen in higher precision.\n",
    "\n",
    "### 2. The Rise of FP8 (e.g., SGLang, Transformer Engine)\n",
    "\n",
    "**FP8** (8-bit floating point) is the latest frontier for H100 (Hopper) GPUs.\n",
    "\n",
    "* **Training:** Libraries like NVIDIA's **Transformer Engine** use FP8 for the forward and backward passes. This provides a massive speed boost (up to 3x over BF16).\n",
    "* **Inference (SGLang):** Frameworks like **SGLang** use FP8 to compress models for serving. Because FP8 has a better dynamic range than INT8, it maintains higher accuracy for LLM inference while drastically reducing the KV cache memory and model size.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Summary: Which one should you use?\n",
    "\n",
    "| Scenario | Recommended Method |\n",
    "| --- | --- |\n",
    "| **Standard GPU (Older)** | `torch.amp` with `float16` + `GradScaler` |\n",
    "| **Modern GPU (A100/H100)** | `torch.amp` with `bfloat16` (No scaler needed) |\n",
    "| **Massive LLM Training** | Megatron-LM or DeepSpeed with `BF16` |\n",
    "| **Cutting-edge H100 Training** | `FP8` via Transformer Engine |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda using torch.bfloat16...\n",
      "Epoch 0: Loss 1.2899\n",
      "Epoch 1: Loss 1.2702\n",
      "Epoch 2: Loss 1.2508\n",
      "Epoch 3: Loss 1.2337\n",
      "Epoch 4: Loss 1.2178\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Setup device and modern AMP components\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# For modern hardware (Ampere+), 'bfloat16' is often better than 'float16'\n",
    "use_dtype = torch.bfloat16 if device == \"cuda\" else torch.float16 # For A100 and 30xx NVIDIA GPU\n",
    "\n",
    "model = nn.Linear(10, 1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# GradScaler is only needed for float16 (usually on CUDA)\n",
    "# It's a no-op if disabled or using bfloat16\n",
    "scaler = torch.amp.GradScaler(device, enabled=(use_dtype == torch.bfloat16)) ##### Important\n",
    "\n",
    "# Dummy data\n",
    "data = torch.randn(64, 10).to(device)\n",
    "target = torch.randn(64, 1).to(device)\n",
    "\n",
    "print(f\"Starting training on {device} using {use_dtype}...\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Autocast context for forward pass\n",
    "    with torch.amp.autocast(device_type=device, dtype=use_dtype): ##### Important\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "    # 3. Scaled Backward pass\n",
    "    # scaler.scale(loss) multiplies loss by a scale factor\n",
    "    scaler.scale(loss).backward() ##### Important\n",
    "\n",
    "    # 4. Scaled Step\n",
    "    # scaler.step() unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer) ##### Important\n",
    "\n",
    "    # 5. Update the scale factor for the next iteration\n",
    "    scaler.update() ##### Important\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (ai-chatbot)",
   "language": "python",
   "name": "ai-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
